{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a862be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Transformers in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (4.32.1)\n",
      "Requirement already satisfied: filelock in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from Transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->Transformers) (2023.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.15.1->Transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from requests->Transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from requests->Transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from requests->Transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from requests->Transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (1.11.1)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from scipy) (1.24.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/15/23/5d44decce68370cfeef76943bca3f2fb5b0057b7ab39f9d3b1feb53d64b8/torch-2.2.1-cp311-none-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading torch-2.2.1-cp311-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/bcsstaff/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Using cached torch-2.2.1-cp311-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-2.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install vaderSentiment\n",
    "#%pip install geopandas\n",
    "#%pip install textblob\n",
    "%pip install Transformers\n",
    "%pip install scipy\n",
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b68700ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast\n",
    "from textblob import TextBlob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import ast\n",
    "#nltk.download('all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0604ea19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | comments                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|---:|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 13 | Unfortunately we weren't able to chat too much with Ali or David, because we were so busy, and they seem lovely!                                                                                                                                                                                                                                                                    |\n",
      "|    | <br/>                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|    | <br/>The suite was a wonderful place to stay, out of the hubbub of really tourist-y areas.  There are great restaurants nearby though, and walking around the area is a cinch.  Make sure to look up bus routes (if you don't have a car) before you get here, because the house isn't on an obvious (frequent) route (although once we knew which busses were best, it was easy).  |\n",
      "|    | <br/>                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|    | <br/>Ali and David are a very good resource, and this was a great place to stay!                                                                                                                                                                                                                                                                                                    |\n",
      "| 14 | We couldn't have asked for much more from our hosts! Ali and David were so welcoming, and the room is so clean and the bed soooo comfy. Our time in Portland was definitely enhanced by staying here. Couldn't recommend it more!                                                                                                                                                   |\n",
      "| 15 | This was a perfect space in a charming, convenient neighborhood. Hosts were helpful but not intrusive. A great find!                                                                                                                                                                                                                                                                |\n",
      "| 16 | Very clean, quiet, friendly hosts.  We enjoyed our two night stay very much.  Recommend highly - great places to eat in the Alberta area.                                                                                                                                                                                                                                           |\n"
     ]
    }
   ],
   "source": [
    "original_data=pd.read_csv('reviews.csv')\n",
    "original_data.head()\n",
    "print(original_data['comments'].iloc[13:17].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93aac808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeTags():\n",
    "    L=[]\n",
    "    with open('reviews.csv','r') as f:\n",
    "        reader=csv.reader(f)\n",
    "        c=0\n",
    "        for row in reader:\n",
    "            X=clean(row[5])\n",
    "            L.append([row[0],row[1],row[2],row[3],row[4],X])\n",
    "    with open('reviews_clean.csv','w') as f2:\n",
    "        writer=csv.writer(f2)\n",
    "        for row in L:\n",
    "            writer.writerow(row)   \n",
    "def clean(s):\n",
    "    try:\n",
    "        X=re.sub('<br/>',\" \",s)\n",
    "        X=re.sub(\"\\n\",\"\",X)\n",
    "        return X.strip()\n",
    "    except:\n",
    "        print(s)\n",
    "    return s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "facba6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "removeTags() #Remove the html tags that are present in the comments field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4b7ed7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | comments                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|---:|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| 13 | Unfortunately we weren't able to chat too much with Ali or David, because we were so busy, and they seem lovely!    The suite was a wonderful place to stay, out of the hubbub of really tourist-y areas.  There are great restaurants nearby though, and walking around the area is a cinch.  Make sure to look up bus routes (if you don't have a car) before you get here, because the house isn't on an obvious (frequent) route (although once we knew which busses were best, it was easy).   Ali and David are a very good resource, and this was a great place to stay! |\n",
      "| 14 | We couldn't have asked for much more from our hosts! Ali and David were so welcoming, and the room is so clean and the bed soooo comfy. Our time in Portland was definitely enhanced by staying here. Couldn't recommend it more!                                                                                                                                                                                                                                                                                                                                               |\n",
      "| 15 | This was a perfect space in a charming, convenient neighborhood. Hosts were helpful but not intrusive. A great find!                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "| 16 | Very clean, quiet, friendly hosts.  We enjoyed our two night stay very much.  Recommend highly - great places to eat in the Alberta area.                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n"
     ]
    }
   ],
   "source": [
    "reviews=pd.read_csv('reviews_clean.csv').dropna(subset=['comments'])\n",
    "print(reviews['comments'].iloc[13:17].to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "efaaf37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalization:\n",
    "    def __init__(self,df,column):\n",
    "        #self.stop_words=[item for item in stopwords.words('english') if item!='not']\n",
    "        self.stop_words=stopwords.words('english')\n",
    "        contraction_patterns = [(r'can\\'t', 'cannot'),\n",
    "                                (r'haven\\'t', 'have not'),\n",
    "                                (r'wasn\\'t', 'was not'),\n",
    "                                (r'isn\\'t', 'is not'),\n",
    "                                (r'didn\\'t', 'did not'),\n",
    "                                (r'hasn\\'t', 'has not'),\n",
    "                                (r'weren\\'t', 'were not')]\n",
    "        self.contraction_regexes = [(re.compile(p), replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        self.wntl=WordNetLemmatizer()\n",
    "        self.tokenizer=RegexpTokenizer(r'\\w+') #Punctuation remover\n",
    "        self.df=df\n",
    "        self.column=column\n",
    "        self.applyAll()\n",
    "    def removeStopWords(self,s):\n",
    "        word_tokens=self.tokenizer.tokenize(s) #Remove Punctuation\n",
    "        return [self.wntl.lemmatize(word) for word in word_tokens if word not in self.stop_words]\n",
    "\n",
    "    def do_contraction_normalization(self, text):\n",
    "        for contraction_regex, replaced_text in self.contraction_regexes:\n",
    "            text = contraction_regex.sub(replaced_text, text)\n",
    "        return text\n",
    "\n",
    "    def normalize(self,text):\n",
    "        cleaned_tokens=self.removeStopWords(self.do_contraction_normalization(text))\n",
    "        return pd.Series([cleaned_tokens,len(cleaned_tokens)])\n",
    "    def applyAll(self):\n",
    "        self.df[[\"Tokens\",'Corpus_Length']]=self.df[self.column].apply(self.normalize)\n",
    "        return self.df\n",
    "    def get(self):\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5859a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Corpus_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12899</td>\n",
       "      <td>68414</td>\n",
       "      <td>2010-07-24</td>\n",
       "      <td>170065</td>\n",
       "      <td>Hope</td>\n",
       "      <td>Unfortunately we weren't able to chat too much...</td>\n",
       "      <td>[Unfortunately, able, chat, much, Ali, David, ...</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12899</td>\n",
       "      <td>71418</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>89041</td>\n",
       "      <td>Ariel</td>\n",
       "      <td>We couldn't have asked for much more from our ...</td>\n",
       "      <td>[We, asked, much, host, Ali, David, welcoming,...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12899</td>\n",
       "      <td>74541</td>\n",
       "      <td>2010-08-06</td>\n",
       "      <td>174803</td>\n",
       "      <td>Marilyn</td>\n",
       "      <td>This was a perfect space in a charming, conven...</td>\n",
       "      <td>[This, perfect, space, charming, convenient, n...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12899</td>\n",
       "      <td>75259</td>\n",
       "      <td>2010-08-07</td>\n",
       "      <td>189922</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Very clean, quiet, friendly hosts.  We enjoyed...</td>\n",
       "      <td>[Very, clean, quiet, friendly, host, We, enjoy...</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id     id        date  reviewer_id reviewer_name  \\\n",
       "13       12899  68414  2010-07-24       170065          Hope   \n",
       "14       12899  71418  2010-07-31        89041         Ariel   \n",
       "15       12899  74541  2010-08-06       174803       Marilyn   \n",
       "16       12899  75259  2010-08-07       189922         Kathy   \n",
       "\n",
       "                                             comments  \\\n",
       "13  Unfortunately we weren't able to chat too much...   \n",
       "14  We couldn't have asked for much more from our ...   \n",
       "15  This was a perfect space in a charming, conven...   \n",
       "16  Very clean, quiet, friendly hosts.  We enjoyed...   \n",
       "\n",
       "                                               Tokens  Corpus_Length  \n",
       "13  [Unfortunately, able, chat, much, Ali, David, ...             50  \n",
       "14  [We, asked, much, host, Ali, David, welcoming,...             20  \n",
       "15  [This, perfect, space, charming, convenient, n...             12  \n",
       "16  [Very, clean, quiet, friendly, host, We, enjoy...             18  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews=TextNormalization(reviews,'comments').get()\n",
    "reviews.iloc[13:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6f77614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sentimentVader:\n",
    "    def __init__(self,df,column):\n",
    "        self.df=df\n",
    "        self.column=column\n",
    "        self.sentimentAnalyzer=SentimentIntensityAnalyzer()\n",
    "        self.applyAll()\n",
    "    def getSentimentVader(self,toks):\n",
    "        s=\" \".join(toks)\n",
    "        #s=toks\n",
    "        sent = self.sentimentAnalyzer.polarity_scores(s)\n",
    "        return pd.Series([float(sent['compound']),float(sent['neg']),float(sent['pos']),float(sent['neu'])])\n",
    "    def applyAll(self):\n",
    "        self.df[['sentimentVader','negVader','posVader','neutralVader']]=self.df[self.column].apply(self.getSentimentVader)\n",
    "    def get(self): \n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d7010809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>comments</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Corpus_Length</th>\n",
       "      <th>sentimentVader</th>\n",
       "      <th>posVader</th>\n",
       "      <th>negVader</th>\n",
       "      <th>neutralVader</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12899</td>\n",
       "      <td>68414</td>\n",
       "      <td>2010-07-24</td>\n",
       "      <td>170065</td>\n",
       "      <td>Hope</td>\n",
       "      <td>Unfortunately we weren't able to chat too much...</td>\n",
       "      <td>[Unfortunately, able, chat, much, Ali, David, ...</td>\n",
       "      <td>50</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12899</td>\n",
       "      <td>71418</td>\n",
       "      <td>2010-07-31</td>\n",
       "      <td>89041</td>\n",
       "      <td>Ariel</td>\n",
       "      <td>We couldn't have asked for much more from our ...</td>\n",
       "      <td>[We, asked, much, host, Ali, David, welcoming,...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12899</td>\n",
       "      <td>74541</td>\n",
       "      <td>2010-08-06</td>\n",
       "      <td>174803</td>\n",
       "      <td>Marilyn</td>\n",
       "      <td>This was a perfect space in a charming, conven...</td>\n",
       "      <td>[This, perfect, space, charming, convenient, n...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.9371</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>12899</td>\n",
       "      <td>75259</td>\n",
       "      <td>2010-08-07</td>\n",
       "      <td>189922</td>\n",
       "      <td>Kathy</td>\n",
       "      <td>Very clean, quiet, friendly hosts.  We enjoyed...</td>\n",
       "      <td>[Very, clean, quiet, friendly, host, We, enjoy...</td>\n",
       "      <td>18</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    listing_id     id        date  reviewer_id reviewer_name  \\\n",
       "13       12899  68414  2010-07-24       170065          Hope   \n",
       "14       12899  71418  2010-07-31        89041         Ariel   \n",
       "15       12899  74541  2010-08-06       174803       Marilyn   \n",
       "16       12899  75259  2010-08-07       189922         Kathy   \n",
       "\n",
       "                                             comments  \\\n",
       "13  Unfortunately we weren't able to chat too much...   \n",
       "14  We couldn't have asked for much more from our ...   \n",
       "15  This was a perfect space in a charming, conven...   \n",
       "16  Very clean, quiet, friendly hosts.  We enjoyed...   \n",
       "\n",
       "                                               Tokens  Corpus_Length  \\\n",
       "13  [Unfortunately, able, chat, much, Ali, David, ...             50   \n",
       "14  [We, asked, much, host, Ali, David, welcoming,...             20   \n",
       "15  [This, perfect, space, charming, convenient, n...             12   \n",
       "16  [Very, clean, quiet, friendly, host, We, enjoy...             18   \n",
       "\n",
       "    sentimentVader  posVader  negVader  neutralVader  \n",
       "13          0.9790     0.392     0.034         0.574  \n",
       "14          0.8689     0.403     0.000         0.597  \n",
       "15          0.9371     0.643     0.000         0.357  \n",
       "16          0.9489     0.562     0.000         0.438  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews=sentimentVader(reviews,'Tokens').get()\n",
    "reviews.iloc[13:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cc1bcc1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class contractionReplacer:\n",
    "    def __init__(self,contraction_patterns):\n",
    "        self.contraction_regexes=[(re.compile(p),replaced_text) for p, replaced_text in contraction_patterns]\n",
    "        \n",
    "    def do_contraction_normalization(self,text):\n",
    "        for contraction_regex, replaced_text in self.contraction_regexes:\n",
    "            text=contraction_regex.sub(replaced_text, text)\n",
    "        return text\n",
    "\n",
    "\n",
    "def PartsOfSpeech(s):\n",
    "    contraction_patterns=[(r'can\\'t','cannot'),\n",
    "                     (r'haven\\'t','have not'),\n",
    "                     (r'wasn\\'t','was not'),\n",
    "                     (r'isn\\'t','is not'),\n",
    "                (r'didn\\'t','did not'),\n",
    "                         (r'hasn\\'t','has not'),\n",
    "                         (r'weren\\'t','were not')]\n",
    "    nouns = []\n",
    "    adjectives = []\n",
    "    adjective_nouns=[]\n",
    "    comma = \"\"\n",
    "    comma2 = \"\"\n",
    "    contractions_replaced=contractionReplacer(contraction_patterns).do_contraction_normalization(s)\n",
    "    pos = TextBlob(contractions_replaced).tags\n",
    "    for i in range(len(pos)):\n",
    "        if 'NN' in pos[i][1]:\n",
    "            nouns.append(pos[i][0])\n",
    "        if 'JJ' in pos[i][1]:\n",
    "            adj=pos[i][0]\n",
    "            if i>1:\n",
    "                for item in pos[i-2:i]:\n",
    "                    if item[0].lower()=='not':\n",
    "                        adj=item[0]+\" \"+adj\n",
    "                        break\n",
    "            if i<len(pos)-1 and 'NN' in pos[i+1][1]:\n",
    "                following=pos[i+1][0]\n",
    "                adjective_nouns.append(adj+\" \"+following)\n",
    "\n",
    "            \n",
    "            adjectives.append(adj)\n",
    "            \n",
    "\n",
    "\n",
    "    return pd.Series([nouns, adjectives, adjective_nouns])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "55b64f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews[['nouns','adjectives','adjectives_with_nouns']]=reviews['comments'].apply(PartsOfSpeech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60688ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After contraction replacement Hello i'm not very happy.  I was not able to eat\n",
      "\n",
      "tokens ['Hello', 'i', 'm', 'not', 'very', 'happy', 'I', 'was', 'not', 'able', 'to', 'eat']\n",
      "\n",
      "New tokens ['Hello', 'happy', 'I', 'able', 'eat']\n"
     ]
    }
   ],
   "source": [
    "class SentimentAnalysis:\n",
    "    def __init__(self,df,column):\n",
    "        self.MODEL=f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.MODEL)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.MODEL)\n",
    "        self.column = column\n",
    "        self.df=self.getAllSentiment(df)\n",
    "\n",
    "\n",
    "\n",
    "    def getSentiment(self,S):\n",
    "        encoded_text = self.tokenizer(S, return_tensors='pt', max_length=512, truncation=True)\n",
    "        output = (self.model(**encoded_text))\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        L = softmax(scores)\n",
    "        L=[str(item)+',' for item in L]\n",
    "        D = {'neg': L[0], 'neu': L[1], 'pos': L[2]}\n",
    "        return \"\".join(L)\n",
    "\n",
    "    def getAllSentiment(self,df):\n",
    "        df['sentimentRoberta']=self.getSentiment(str(df[self.column]))\n",
    "        return df\n",
    "    def get(self):\n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b0878",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews=SentimentAnalysisRoberta(reviews,'comments').get()\n",
    "reviews[['negRoberta', 'neutralRoberta','posRoberta','nothing']] = reviews['sentimentRoberta'].str.split(' ', expand=True)\n",
    "reviews=reviews.drop(columns=['sentimentRoberta','nothing'])\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44ab0b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not']\n"
     ]
    }
   ],
   "source": [
    "listings=pd.read_csv('listingsDetailed.csv')\n",
    "print(listings.info())\n",
    "print(listings['name'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf22fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def splitNameCol(s):\n",
    "    cats=[item.strip() for item in s.split('·')]\n",
    "    name=cats[0]\n",
    "    \n",
    "    rating,bedrooms,beds,baths=\"No Rating\",None,None,None\n",
    "    for item in cats[1:]:\n",
    "        if 'room' in item:\n",
    "            bedrooms=item.split(' ')[0]\n",
    "        elif 'Studio' in item:\n",
    "            bedrooms=0.5\n",
    "        elif 'bed' in item and 'room' not in item:\n",
    "            beds=item.split(' ')[0]\n",
    "        elif 'bath' in item:\n",
    "            baths=item.split(' ')[0]\n",
    "        elif '★' in item:\n",
    "            rating=item.split('★')[1]\n",
    "   \n",
    "    return pd.Series([name, rating,bedrooms, beds, baths])\n",
    "                \n",
    "def convertTypeFloat(x):\n",
    "    try:\n",
    "        x=float(x)\n",
    "    except:\n",
    "        x=np.nan\n",
    "    return x\n",
    "\n",
    "def convertPrice(x):\n",
    "    try:\n",
    "        x=float(x.split('$')[1])\n",
    "    except:\n",
    "        x=np.nan\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d169995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "listings[['Name', 'Rating','Bedrooms','Beds','Bathrooms']] = listings['name'].apply(splitNameCol)\n",
    "listings['price']= listings['price'].apply(convertPrice)\n",
    "listings['Rating']= listings['Rating'].apply(convertTypeFloat)\n",
    "listings['Bedrooms'] = listings['Bedrooms'].apply(convertTypeFloat)\n",
    "listings['Bathrooms'] = listings['Bathrooms'].apply(convertTypeFloat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ecf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=pd.merge(reviews,listings,left_on='listing_id', right_on='id',how='inner')\n",
    "merged = merged[['listing_id', 'host_id', 'reviewer_id', 'host_name','Name', 'comments', 'sentimentVader', 'posVader', 'negVader',\n",
    "             'neutralVader', 'negRoberta', 'neutralRoberta', 'posRoberta', 'Rating', 'nouns','adjectives_with_nouns', 'adjectives_' 'neighbourhood',\n",
    "             'number_of_reviews', 'price', 'accommodates', 'Bedrooms', 'Bathrooms','Beds', 'reviews_per_month',\n",
    "             'review_scores_checkin', 'review_scores_communication', 'review_scores_location', 'review_scores_value',\n",
    "             'date', 'property_type', 'room_type','id_x']]\n",
    "\n",
    "merged = merged.rename(columns={'Rating': 'host_rating','date':'date_reviewed','id_x':'id'})\n",
    "merged['date_reviewed']=pd.to_datetime(merged['date_reviewed']).dt.date\n",
    "merged.to_csv('airbnbData.csv')\n",
    "merged.to_pickle('airbnbData.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b11bf738",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'airbnbData.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reviews_df\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairbnbData.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(reviews_df\u001b[38;5;241m.\u001b[39minfo())\n\u001b[1;32m      4\u001b[0m reviews_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnouns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mreviews_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnouns\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: ast\u001b[38;5;241m.\u001b[39mliteral_eval(x))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/pickle.py:179\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    180\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    182\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    183\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    184\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    185\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:868\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    860\u001b[0m             handle,\n\u001b[1;32m    861\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    864\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    865\u001b[0m         )\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    869\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'airbnbData.pkl'"
     ]
    }
   ],
   "source": [
    "reviews_df=pd.read_pickle('airbnbData.pkl')\n",
    "print(reviews_df.info())\n",
    "\n",
    "reviews_df['nouns']=reviews_df['nouns'].apply(lambda x: ast.literal_eval(x))\n",
    "reviews_df['adjectives']=reviews_df['adjectives'].apply(lambda x: ast.literal_eval(x))\n",
    "print(reviews_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dcec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
